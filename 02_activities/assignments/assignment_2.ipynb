{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "\n",
    "# Use the relative path from the 'assignments' directory\n",
    "adult_dt = (pd.read_csv('../../05_src/data/adult/adult.data', header=None, names=columns)\n",
    "              .assign(income=lambda x: (x.income.str.strip() == '>50K') * 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (22792, 14)\n",
      "X_test shape: (9769, 14)\n",
      "Y_train shape: (22792,)\n",
      "Y_test shape: (9769,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = adult_dt.drop('income', axis=1)  # All columns except income\n",
    "Y = adult_dt['income']  # Only the income column\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Check the shapes to confirm\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random state in a data splitting function (like train_test_split in scikit-learn) is a parameter that controls the randomness of the data split. Every time you run the code, with the same data and the same random state, you get the exact same split into training and test sets.\n",
    "\n",
    "It is useful for:\n",
    "Reproducibility: It ensures that anyone running the code gets the same results, especially, when youâ€™re collaborating, sharing results, or debugging.\n",
    "\n",
    "Consistency in Model Evaluation: When testing different models or model parameters, it evaluates each model on the same split of training and testing data.\n",
    "By setting random state, one ensures that all models are evaluated on the exact same test set, making comparisons fair and consistent.\n",
    "\n",
    "Easier Experimentation: With a fixed random state, one can try different algorithms, parameters, or feature sets without worrying about differences in training/testing splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed X shape: (32561, 107)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "adult_dt = pd.read_csv('../../05_src/data/adult/adult.data', header=None, names=columns)\n",
    "adult_dt['income'] = adult_dt['income'].str.strip()\n",
    "\n",
    "# Separate features and target\n",
    "X = adult_dt.drop('income', axis=1)\n",
    "Y = adult_dt['income']\n",
    "\n",
    "# Define pipelines\n",
    "numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Pipeline for numerical features\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('knn_imputer', KNNImputer(n_neighbors=7, weights=\"distance\")),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('simple_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore', drop='if_binary'))\n",
    "])\n",
    "\n",
    "# Column Transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Print shape to confirm\n",
    "print(\"Transformed X shape:\", X_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions: ['<=50K' '<=50K' '>50K' '<=50K' '<=50K' '>50K' '>50K' '<=50K' '<=50K'\n",
      " '>50K']\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the model pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),  # Column Transformer for preprocessing\n",
    "    ('classifier', RandomForestClassifier())  # RandomForest classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = model_pipeline.predict(X_test)\n",
    "\n",
    "# Optional: Print sample predictions or evaluate model performance\n",
    "print(\"Sample Predictions:\", predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-Level Cross-Validation Results:\n",
      "    fit_time  score_time  test_neg_log_loss  train_neg_log_loss  test_roc_auc  \\\n",
      "0  25.782416    0.721329          -0.349779           -0.081280      0.904580   \n",
      "1  26.025859    0.605199          -0.361224           -0.081217      0.902057   \n",
      "2  27.426582    0.616514          -0.376189           -0.081322      0.900801   \n",
      "3  25.348829    0.582020          -0.357874           -0.082438      0.906503   \n",
      "4  26.431999    0.766695          -0.360224           -0.081852      0.902474   \n",
      "\n",
      "   train_roc_auc  test_accuracy  train_accuracy  test_balanced_accuracy  \\\n",
      "0            1.0       0.849528        0.999890                0.772531   \n",
      "1            1.0       0.846896        1.000000                0.767414   \n",
      "2            1.0       0.851689        0.999945                0.773809   \n",
      "3            1.0       0.861562        1.000000                0.785248   \n",
      "4            1.0       0.853006        1.000000                0.774677   \n",
      "\n",
      "   train_balanced_accuracy  \n",
      "0                 0.999774  \n",
      "1                 1.000000  \n",
      "2                 0.999887  \n",
      "3                 1.000000  \n",
      "4                 1.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = {\n",
    "    'neg_log_loss': 'neg_log_loss',       # Negative log loss\n",
    "    'roc_auc': 'roc_auc',                 # ROC AUC\n",
    "    'accuracy': 'accuracy',               # Accuracy\n",
    "    'balanced_accuracy': 'balanced_accuracy'  # Balanced accuracy\n",
    "}\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cv_results = cross_validate(\n",
    "    model_pipeline, \n",
    "    X_train, Y_train, \n",
    "    cv=5, \n",
    "    scoring=scoring,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Convert cross-validation results to a DataFrame for easier interpretation\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Display the fold-level results for training and validation metrics\n",
    "print(\"Fold-Level Cross-Validation Results:\")\n",
    "print(cv_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-Level Cross-Validation Results Sorted by Test Negative Log Loss:\n",
      "    fit_time  score_time  test_neg_log_loss  train_neg_log_loss  test_roc_auc  \\\n",
      "0  25.782416    0.721329          -0.349779           -0.081280      0.904580   \n",
      "3  25.348829    0.582020          -0.357874           -0.082438      0.906503   \n",
      "4  26.431999    0.766695          -0.360224           -0.081852      0.902474   \n",
      "1  26.025859    0.605199          -0.361224           -0.081217      0.902057   \n",
      "2  27.426582    0.616514          -0.376189           -0.081322      0.900801   \n",
      "\n",
      "   train_roc_auc  test_accuracy  train_accuracy  test_balanced_accuracy  \\\n",
      "0            1.0       0.849528        0.999890                0.772531   \n",
      "3            1.0       0.861562        1.000000                0.785248   \n",
      "4            1.0       0.853006        1.000000                0.774677   \n",
      "1            1.0       0.846896        1.000000                0.767414   \n",
      "2            1.0       0.851689        0.999945                0.773809   \n",
      "\n",
      "   train_balanced_accuracy  \n",
      "0                 0.999774  \n",
      "3                 1.000000  \n",
      "4                 1.000000  \n",
      "1                 1.000000  \n",
      "2                 0.999887  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assume `cv_results` is the output from `cross_validate` function\n",
    "# Convert the cross-validation results to a DataFrame\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Sort the DataFrame by 'test_neg_log_loss' in descending order\n",
    "# (if we want higher values of negative log loss to be at the top)\n",
    "cv_results_df_sorted = cv_results_df.sort_values(by='test_neg_log_loss', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(\"Fold-Level Cross-Validation Results Sorted by Test Negative Log Loss:\")\n",
    "print(cv_results_df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validation Results:\n",
      "fit_time                   26.203137\n",
      "score_time                  0.658351\n",
      "test_neg_log_loss          -0.361058\n",
      "train_neg_log_loss         -0.081622\n",
      "test_roc_auc                0.903283\n",
      "train_roc_auc               1.000000\n",
      "test_accuracy               0.852536\n",
      "train_accuracy              0.999967\n",
      "test_balanced_accuracy      0.774736\n",
      "train_balanced_accuracy     0.999932\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of each metric across folds\n",
    "mean_results = cv_results_df.mean()\n",
    "\n",
    "# Display the mean results\n",
    "print(\"Mean Cross-Validation Results:\")\n",
    "print(mean_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance Metrics:\n",
      "{'neg_log_loss': -0.3764444654687329, 'roc_auc': 0.9006139400505597, 'accuracy': 0.8504452861091207, 'balanced_accuracy': 0.7674587716445606}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Get predicted probabilities and class predictions for X_test\n",
    "Y_pred_proba = model_pipeline.predict_proba(X_test)  # Probability predictions for each class\n",
    "Y_pred = model_pipeline.predict(X_test)              # Class label predictions\n",
    "\n",
    "# Calculate each metric on the test set\n",
    "test_metrics = {\n",
    "    'neg_log_loss': -log_loss(Y_test, Y_pred_proba),               # Negative log loss\n",
    "    'roc_auc': roc_auc_score(Y_test, Y_pred_proba[:, 1]),          # ROC AUC (using only positive class probabilities)\n",
    "    'accuracy': accuracy_score(Y_test, Y_pred),                    # Accuracy\n",
    "    'balanced_accuracy': balanced_accuracy_score(Y_test, Y_pred)   # Balanced accuracy\n",
    "}\n",
    "\n",
    "# Display the results as a dictionary\n",
    "print(\"Test Set Performance Metrics:\")\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recoding the target variable income immediately after loading the data is convenient for several reasons:\n",
    "\n",
    "1. Standardizes the Target Variable for Modeling\n",
    "\n",
    "2. Simplifies Later Processing and Analysis\n",
    "\n",
    "3. Improves Readability and Reduces Code Duplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
